---
title: "Workflow example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Workflow example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This vignette presents a more in-depth example of using priorsense
with `brms`. It demonstrates how to select different priors to power-scale, and which quantities to check the sensitivity of.

```{r}
library(brms)
library(priorsense)
library(posterior)

options(priorsense.plot_help_text = FALSE)
options(brms.backend = "rstan")
```

We use the `airquality` data set, and fit a model predicting the amount of ozone based on the temperature.

```{r}
data(airquality)
aq <- na.omit(airquality)
```

# Linear model

We first fit a linear model, with some priors specified. For each of the priors we add a `tag` as these will be used for selectively power-scaling the priors.

```{r}
fit_lin <- brm(
  formula = bf(Ozone ~ Temp),
  data = aq,
  family = gaussian(),
  prior = c(
    prior(normal(0, 5), class = "b", coef = "Temp", tag = "b"),
    prior(normal(0, 10), class = "sigma", tag = "sigma"),
    prior(normal(0, 100), class = "Intercept", tag = "intercept")
  )
)
```

We will work with the posterior draws, rather than the entire `brmsfit` object. This requires binding the `log_lik` to the draws object.
```{r}
post_draws_lin <- as_draws_df(fit_lin) |>
  bind_draws(log_lik_draws(fit_lin))
```

As the model is simple, and each parameter is directly interpretable, we can perform the power-scaling sensitivity check directly on all the parameters.

```{r}
powerscale_sensitivity(
  post_draws_lin
)
```

This indicates that when we power-scale all the priors, the posterior of sigma is changing. We could also look at predictive metrics, such as Bayesian R2 and log-score.


We first define a log-score function.

```{r}
logscore <- function(x) {
  as.matrix(rowSums(log_lik(x)))
}
```

And then bind the draws of predictive metrics to the posterior draws.

```{r}
post_draws_lin <- post_draws_lin |>
  bind_draws(
    predictions_as_draws(
      x = fit_lin, predict_fn = bayes_R2, prediction_names = "R2", summary = FALSE)) |>
  bind_draws(
    predictions_as_draws(
      x = fit_lin, predict_fn = logscore, prediction_names = "logscore"))

powerscale_sensitivity(
  post_draws_lin,
)
```

In this case, these predictive metrics do not appear to be sensitive
to power-scaling the prior. If we are focused on prediction, we might
not be concerned about the sensitivity in the sigma
parameter. However, as the model is simple and sigma is interpretable
we can continue investigation.

We can confirm that it is the sigma prior that is causing the possible
prior-data conflict, by using the `prior_selection` argument and
specifying the `tag` we defined when creating the priors.

```{r}
powerscale_sensitivity(
  post_draws_lin,
  prior_selection = "sigma"
)
```

And we can visualise the conflict.

```{r, message = F, warning = F, fig.width = 12, fig.height = 4}
powerscale_plot_dens(
  post_draws_lin,
  variable = "sigma",
  prior_selection = "sigma"
)
```

Here we can see that there is a tendency for the posterior of sigma to
shift closer to zero when the prior is strengthened (power-scaling
alpha > 1). In this case, we did not have strong prior information
that informed the prior, so we can consider a wider prior for sigma.



```{r}
fit_lin2 <- brm(
  formula = bf(Ozone ~ Temp),
  data = aq,
  family = gaussian(),
  prior = c(
    prior(normal(0, 5), class = "b", coef = "Temp", tag = "b"),
    prior(normal(0, 30), class = "sigma", tag = "sigma"),
    prior(normal(0, 100), class = "Intercept", tag = "intercept")
  ),
  backend = "cmdstanr"
)

post_draws_lin2 <- as_draws_df(fit_lin2) |>
  bind_draws(
    predictions_as_draws(
      x = fit_lin2,
      predict_fn = bayes_R2,
      prediction_names = "R2",
      summary = FALSE)
  ) |>
  bind_draws(
    predictions_as_draws(
      x = fit_lin2,
      predict_fn = logscore,
      prediction_names = "logscore")
  ) |>
  bind_draws(log_lik_draws(fit_lin2))
```

```{r}
powerscale_sensitivity(
  post_draws_lin2,
  variable = c("b_Temp", "b_Intercept", "sigma", "R2", "logscore"),
  prior_selection = "sigma"
)

```{r, message = F, warning = F, fig.width = 12, fig.height = 4}
powerscale_plot_dens(
  post_draws_lin2,
  variable = "sigma",
  prior_selection = "sigma"
)
```

The prior sensitivity and prior-data conflict are no longer evident
with a wider prior on sigma.

# Spline model

Next we extend our model by adding splines.

```{r}
fit_spline1 <- brm(
  formula = bf(Ozone ~ s(Temp)),
  data = aq,
  family = gaussian(),
  prior = c(
    prior(normal(0, 5), class = "b", coef = "sTemp_1", tag = "b"),
    prior(normal(0, 10), class = "sds", coef = "s(Temp)", tag = "sds"),
    prior(normal(0, 30), class = "sigma", tag = "sigma"),
    prior(normal(0, 100), class = "Intercept", tag = "intercept")
  ),
  backend = "cmdstanr"
)
```

Now that the model is more complex, it is more important to focus on
checking the sensitivity of specific quantities. Here we will focus on
the log-score and Bayesian R2.


```{r}
post_draws_spline1 <- as_draws_df(fit_spline1) |>
  bind_draws(
    predictions_as_draws(
      x = fit_spline1,
      predict_fn = bayes_R2,
      prediction_names = "R2", summary = FALSE)
  ) |>
    bind_draws(
      predictions_as_draws(
        x = fit_spline1,
        predict_fn = logscore,
        prediction_names = "logscore")
    ) |>
  bind_draws(log_lik_draws(fit_spline1))
```

We start with power-scaling all priors, but only looking at the effect
on `R2` and `logscore`.

```{r}
powerscale_sensitivity(
  post_draws_spline1,
  variable = c("R2", "logscore")
)
```

We see sensitivity in both measures. Next, we selectively power-scale
different priors by specifying the corresponding `tag` in
`prior_selection`.

As this model introduced a prior on the `sds` term, we can start
there.

```{r}
powerscale_sensitivity(
  post_draws_spline1,
  variable = c("R2", "logscore"),
  prior_selection = "sds"
)
```

And we can check all the other priors at once by providing a vector as
the `prior_selection` argument.

```{r}
powerscale_sensitivity(
  post_draws_spline1,
  variable = c("R2", "logscore"),
  prior_selection = c("intercept", "sigma", "b")
)
```

We can visualise the effect of `R2` and `logscore` of power-scaling
the `sds` prior.

```{r, message = F, warning = F, fig.width = 12, fig.height = 4}
powerscale_plot_dens(
  post_draws_spline1,
  variable = c("R2", "logscore"),
  prior_selection = "sds",
  auto_alpha_range = TRUE
)
```

Although not extremely sensitive, there is a tendency for the measures
to decrease as the prior is strengthened, indicating that perhaps the
prior is too strong.

Again, as the prior choice was not well informed by previous
knowledge, we can adjust it.

```{r}
fit_spline2 <- brm(
  formula = bf(Ozone ~ s(Temp)),
  data = aq,
  family = gaussian(),
  prior = c(
    prior(normal(0, 1), class = "b", coef = "sTemp_1", tag = "b"),
    prior(normal(0, 30), class = "sds", coef = "s(Temp)", tag = "sds"),
    prior(normal(0, 30), class = "sigma", tag = "sigma"),
    prior(normal(30, 30), class = "Intercept", tag = "intercept")
  ),
  backend = "cmdstanr"
)
```

```{r}
post_draws_spline2 <- as_draws_df(fit_spline2) |>
  bind_draws(
    predictions_as_draws(
      x = fit_spline2,
      predict_fn = bayes_R2,
      prediction_names = "R2",
      summary = FALSE)
  ) |>
    bind_draws(
      predictions_as_draws(
        x = fit_spline2,
        predict_fn = logscore,
        prediction_names = "logscore")
    ) |>
  bind_draws(log_lik_draws(fit_spline2))
```

And the conflict is resolved.

```{r}
powerscale_sensitivity(
  post_draws_spline2,
  variable = c("R2", "logscore"),
  prior_selection = "sds"
)
```

```{r, message = F, warning = F, fig.width = 12, fig.height = 4}
powerscale_plot_dens(
  post_draws_spline2,
  variable = c("R2", "logscore"),
  prior_selection = "sds",
  auto_alpha_range = TRUE
)
```
